{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import print_function\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "from torchvision import datasets, transforms\n", "import numpy as np\n", "import torch.utils.data as utils\n", "import time"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Encoder(nn.Module):\n", "    def __init__(self, inputt, hidden):\n", "        super(Encoder, self).__init__()\n", "        self.inputt = inputt\n", "        self.hidden = hidden\n", "        self.fca = nn.Linear(inputt, inputt)\n", "        self.fc1 = nn.Linear(inputt, hidden)\n", "    def forward(self, x, thresh):\n", "        x = x.view(-1, self.inputt)\n", "        x_a = F.sigmoid(self.fca(x))\n", "        x_a = (x_a > thresh).float()\n", "        x = torch.mul(x, x_a)\n", "        return F.relu(self.fc1(x))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Decoder(nn.Module):\n", "    def __init__(self, inputt, hidden):\n", "        super(Decoder, self).__init__()\n", "        self.inputt = inputt\n", "        self.hidden = hidden\n", "        self.fc1 = nn.Linear(inputt, hidden)\n", "    def forward(self, x):\n", "        return F.relu(self.fc1(x))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class AutoEncoder(nn.Module):\n", "    def __init__(self, inputt=12, hidden=200):\n", "        super(AutoEncoder, self).__init__()\n", "        self.inputt = inputt\n", "        self.hidden = hidden\n", "        self.fc1 = Encoder(inputt, hidden)\n", "        self.fc2 = Decoder(hidden, inputt)\n", "    def forward(self, x, thresh):\n", "        return self.fc2(self.fc1(x, thresh))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Classifier(nn.Module):\n", "    def __init__(self, inputt=200,out=3):\n", "        super(Classifier, self).__init__()\n", "        self.inputt = inputt\n", "        self.out = out\n", "        self.fc1 = nn.Linear(inputt, out)\n", "    def forward(self, x):\n", "        return F.softmax(self.fc1(x))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def kl_divergence(p, q):\n", "    p = F.softmax(p)\n", "    q = F.softmax(q)\n", "    return torch.sum(p * torch.log(p / q)) + torch.sum((1 - p) * torch.log((1 - p) / (1 - q)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train(model, model2, device, train_loader, optimizer, optimizer2, epoch, log_interval, sparsity_param, thresh, inp_size, batch_size):\n", "    model.train()\n", "    eff_number_of_sensors = []\n", "    train_loss = 0\n", "    correct = 0\n", "    for batch_idx, (data, label) in enumerate(train_loader):  #Itererate over the training data in batches\n", "        data, label = data.to(device), label.to(device) # copy train data to either GPU or CPU\n", "        optimizer.zero_grad() # Set grad to zero\n", "        optimizer2.zero_grad() # Set grad to zero\n", "        output = model(data, thresh)  # forward propagation\n", "        target = data.view(-1, inp_size)\n", "        attention = F.sigmoid(model.fc1.fca(data.view(-1, inp_size)))\n", "        encoded = model.fc1(torch.mul(data.view(-1, inp_size),attention), thresh)\n", "        ########## classify the encodings #######\n", "        output2 = model2(encoded)  # forward propagation\n", "        pred = output2.max(1, keepdim=True)[1] # get the index of the max log-probability\n", "        correct += pred.eq(label.view_as(pred)).sum().item()\n", "        loss2 = F.nll_loss(output2, label) # Negative Log likelihood loss\n", "        loss2.backward(retain_graph=True) # Error Backpropagation\n", "        optimizer2.step() # update weights\n", "        ###############################################\n", "        rho_hat = torch.sum(encoded, dim=0, keepdim=True)/len(target)\n", "        loss = F.mse_loss(output, target) + 5*kl_divergence(sparsity_param,  rho_hat.cpu()).to(device) + 0.001*torch.sum(attention.to(device))/(batch_size *inp_size)\n", "        train_loss += F.mse_loss(output, target, reduction='sum').item()\n", "        eff_number_of_sensors.append((torch.sum((attention>thresh).float())/(len(target))).cpu().detach().numpy())\n", "        loss.backward() # Error Backpropagation\n", "        optimizer.step() # update weights\n", "        if batch_idx % log_interval == 0:  # for printing loss\n", "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n", "                epoch, batch_idx * len(data), len(train_loader.dataset),\n", "                100. * batch_idx / len(train_loader), loss.item()))\n", "    acc = 100. * correct / len(train_loader.dataset)\n", "    print('\\nTrain accuracy:', acc)\n", "    train_loss /= (len(train_loader)*inp_size)\n", "    print('\\nTrain set: Average loss:', train_loss)\n", "    return train_loss, np.mean(eff_number_of_sensors), acc"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def test(model, model2, device, test_loader, thresh, inp_sz):\n", "    model.eval()\n", "    test_loss = 0\n", "    test_loss2 = 0\n", "    correct = 0\n", "    probs = []\n", "    eff_number_of_sensors_test = []\n", "    time_array = []\n", "    with torch.no_grad():\n", "        for data, label in test_loader:\n", "            data, label = data.to(device), label.to(device)  # copy test data to either GPU or CPU\n", "            output = model(data, thresh)  # forward propagation\n", "            target = data.view(-1, inp_sz)\n", "            start = time.time()\n", "            attention = F.sigmoid(model.fc1.fca(data.view(-1, inp_sz)))\n", "            encoded = model.fc1(torch.mul(data.view(-1, inp_sz), attention), thresh)\n", "            #############################################################\n", "            ## Classify the encodings #################################\n", "            # start = time.time()\n", "            output2 = model2(encoded)  # forward propagation\n", "            end = time.time()\n", "            diff = end - start\n", "            time_array.append(diff)\n", "            test_loss2 += F.nll_loss(output2, label, reduction='sum').item()  # sum up batch loss\n", "            pred = output2.max(1, keepdim=True)[1]  # get the index of the max log-probability\n", "            correct += pred.eq(label.view_as(pred)).sum().item()\n", "            ############################################################\n", "            probs.append(output2.cpu().detach().numpy())\n", "            eff_number_of_sensors_test.append(\n", "                (torch.sum((attention > thresh).float()) / (len(target))).cpu().detach().numpy())\n", "            test_loss += F.mse_loss(output, target, reduction='sum').item()  # sum up batch loss\n", "    test_loss /= (len(test_loader) * inp_sz)\n", "    acc = 100. * correct / len(test_loader.dataset)\n", "    print('\\nTest set: Average loss:', test_loss)\n", "    print('\\nTest accuracy:', acc)\n", "    print('time:', np.mean(time_array))\n", "    return test_loss, np.mean(eff_number_of_sensors_test), acc"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def getData(filename):\n", "    with open(filename) as f:\n", "        data = f.readlines()\n", "    dataset = []\n", "    label = []\n", "    for i in range(len(data)):\n", "        tmp = data[i].split(',')\n", "        if tmp[-1] == 'g\\n':\n", "            label.append(1)\n", "        else:\n", "            label.append(0)\n", "        tmp = [float(tmp[i]) for i in range(len(tmp) - 1)]\n", "        dataset.append(tmp)\n", "    return dataset, label"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def convert2Tensor(trainData, trainLabel, testData, testLabel, batch_size, kwargs):\n", "    # training data\n", "    tensor_x = torch.stack([torch.Tensor(i) for i in trainData])  # transform to torch tensors\n", "    tensor_y = torch.Tensor(trainLabel).long()\n", "    my_dataset = utils.TensorDataset(tensor_x, tensor_y)  # create your datset\n", "    train_loader = utils.DataLoader(my_dataset, batch_size=batch_size, shuffle=True, **kwargs)  # create your dataloader\n\n", "    # test data\n", "    tensor_x = torch.stack([torch.Tensor(i) for i in testData])  # transform to torch tensors\n", "    tensor_y = torch.Tensor(testLabel).long()\n", "    my_dataset = utils.TensorDataset(tensor_x, tensor_y)  # create your dataset\n", "    test_loader = utils.DataLoader(my_dataset, batch_size=batch_size, shuffle=False, **kwargs)  #\n", "    return train_loader, test_loader"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def split_data(dataset, label):\n", "    # train-test split\n", "    indices = np.random.permutation(len(dataset))\n", "    training_idx, test_idx = indices[:280], indices[280:]\n", "    x_train = [dataset[i] for i in training_idx]\n", "    x_test = [dataset[i] for i in test_idx]\n", "    y_train = [label[i] for i in training_idx]\n", "    y_test = [label[i] for i in test_idx]\n", "    return x_train, y_train, x_test, y_test"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main():\n", "    #################################################\n", "    ### Training settings ###########################\n", "    inp_size = 34\n", "    hidden_size = 30\n", "    numClasses = 2\n", "    batch_size = 5\n", "    epochs = 2000\n", "    lr = 0.01\n", "    seed = 1\n", "    log_interval = 10\n", "    use_cuda = False\n", "    rho = 0.5\n", "    thresh = 0.1\n", "    param = {'inp_size': inp_size,'hidden_size': hidden_size, 'batch_size': batch_size, 'numClasses': numClasses, 'epochs': epochs, 'lr': lr, 'rho': rho, 'thresh': thresh}\n", "    sparsity_param = torch.FloatTensor([rho for _ in range(hidden_size)]).unsqueeze(0)\n\n", "    ##################################################\n", "    ## For reproduceable results #####################\n", "    # torch.manual_seed(seed)\n", "    ########## Choosing GPU or CPU ######################\n", "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n", "    #############  Data Loader ##############\n", "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n", "    ####Training\n", "    ###########################################################\n", "    filename = '/data/ionosphere.txt'\n", "    dataset, label = getData(filename)\n", "    x_train, y_train, x_test, y_test = split_data(dataset, label)\n", "    print('==============Training================')\n", "    print('number of class 1: ', y_train.count(0))\n", "    print('number of class 2: ', y_train.count(1))\n", "    print('==============Testing================')\n", "    print('number of class 1: ', y_test.count(0))\n", "    print('number of class 2: ', y_test.count(1))\n", "    train_loader, test_loader = convert2Tensor(x_train, y_train, x_test, y_test, batch_size, kwargs)\n", "    #############################################################\n", "    ############# Instantiate Model #############################\n", "    model = AutoEncoder(inp_size, hidden_size).to(device)\n", "    print(model)\n", "    classifier = Classifier(hidden_size, numClasses).to(device)\n", "    print(classifier)\n", "    ######################### Define optimization #####################\n", "    optimizer  = optim.SGD(model.parameters(), lr=lr)\n", "    optimizer_classifier = optim.SGD(classifier.parameters(), lr=lr)\n", "    ###################################################################\n", "    ### Epoch training\n", "    loss_train_arr = []\n", "    loss_test_arr = []\n", "    sensors_train = []\n", "    sensors_test = []\n", "    test_acc = []\n", "    train_acc = []\n", "    for epoch in range(1, epochs + 1):\n", "        loss_train, no_sensors_train, acc_train = train(model, classifier, device, train_loader, optimizer,\n", "                                                              optimizer_classifier, epoch, log_interval, sparsity_param,\n", "                                                              thresh, inp_size, batch_size)\n", "        loss_test, no_sensors_test, acc = test(model, classifier, device, test_loader, thresh,\n", "                                                                     inp_size)\n", "        ## Store Metrics\n", "        loss_train_arr.append(loss_train)\n", "        loss_test_arr.append(loss_test)\n", "        sensors_train.append(no_sensors_train)\n", "        sensors_test.append(no_sensors_test)\n", "        test_acc.append(acc)\n", "        train_acc.append(acc_train)\n", "    return loss_train_arr, loss_test_arr, sensors_train, sensors_test, train_acc, test_acc"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    loss_train_arr, loss_test_arr, sensors_train, sensors_test, train_acc, test_acc = main()\n", "    print('Train Accuracy', np.max(train_acc))\n", "    print('Test Accuracy', np.max(test_acc))\n", "    idx = np.argmax(test_acc)\n", "    print('index is:', idx)\n", "    print('Number of train sensors', sensors_train[idx])\n", "    print('Number of test sensors', sensors_test[idx])\n", "    print('Train recon loss', loss_train_arr[idx])\n", "    print('Test recon loss', loss_test_arr[idx])"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}